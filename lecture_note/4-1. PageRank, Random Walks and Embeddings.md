# 4-1. PageRank, Random Walks and Embeddings



### This page will investigate graph analysis and learn from matrix

* By treating graph as matrix
  * Can determine importance of node via **Random Walk** >> **PageRank**
  * Obtain node embedding via **Matrix Factorization**
  * Other node embeddings
* Random Walk, Matrix Factorization, Node embeddings are related!!



###  View Web as graph

* Node : individual web page

* Edge : Hyperlink
* Other networks would be Citation network or Reference network



### Questions

* When we look at Web as directe graph, we can ask some questions
  * With given node $v$, what node can $v$ reach?
  * What other nodes can reach $v$?
  * Does all $v$ (web pages) are equally important?
    * FYI answer is NO.



#### So we will cover link analysis approaches to compute importance of node

* PageRank
* Personalized PageRank (PPR)
* Random Walk with Restarts



-----



### PageRank : The "Flow" Model

>  To compute importance, we need to decide what counts as importance. At this point, we can come up with idea to **Links as votes**.
>
>  When we view links as votes, there are 2 links, In-coming and Out-going links, but intuitively we can think that in-coming links are more important. And that's what we are going to veiw as vote.
>
>  But, not all in-coming links are equally important. So we are going to calculate votes(importance) recursively.



#### Importance calculation

<img width="300" alt="image" src="https://user-images.githubusercontent.com/84625523/218099790-78d98411-329d-4a1c-b1c2-70496b842c0f.png">

* Each link's vote is proportional to the **importance** of its source page
  * If page $i$ with importance $r_i$ has $d_i$ out-links, each link gets $r_i/d_i$  votes.
  * Page $j$'s own importance $r_j$ is the sum of votes on its in-links

* A page is important if it is pointed by other important pages



#### Define and calculate rank $r_j$ for node $j$

<img width="268" alt="image" src="https://user-images.githubusercontent.com/84625523/218102666-5c24734c-bbe9-4517-8c1f-536ea5d485b4.png">



* Flow equations is

$$
r_y = r_y/2 + r_a/2
\\
r_a = r_y/2 + r_m
\\
r_m = r_a/2
$$



* and "rank" $r_j$ for node $j$ is

$$
r_j = \sum_{i\rightarrow j}\frac{r_i}{d_i}
\\
\\\text{$d_i$ is out-degree of node $i$}
$$

As you can see in Flow equation, you might wonder use Gaussian elimination to solve this sysrem of linear equation. But, It's not sclable.

So we are going to work this through **Matrix Formulation**



----



### Matrix Formulation

<img width="229" alt="image" src="https://user-images.githubusercontent.com/84625523/218104168-a36c4588-c64a-4942-a6d8-019bcb63d7ff.png">

* **Stochastic Adjacency matrix $M$**
  * $d_i$ is the outdegree of node $i$
  * if $i \rightarrow j$, then $M_{ij} = \frac{1}{d_i}$
    * $M$ is a **column stochastic matrix**, Columns sum to 1
* **Rank vector $r$**
  * An entry per page
  * $r_i$ is the importance score of page $i$
  * $\sum_ir_i = 1$
* So the Flow equation can be written
  * $r = M \cdot r$



#### Example

<img width="769" alt="image" src="https://user-images.githubusercontent.com/84625523/218104268-817de692-340c-451a-97a0-564e81da8efa.png">

#### Connection to Random Walk

* **Imagine Random web Surfer**

  1. At any time $t$, surfer is on some page $i$

  2. At time $t+1$, the surfer follows an out-link from $i$ uniformly at random
  3. Ends up on some page $j$ linked from $i$.
  4. 1~3 happens indefinitely

* Let :

  * $p(t)$ vector whose $i^{th}$ coordinate is the probabilty that the surfer is at page $i$ at time $t$.
  * So, $p(t)$ is a probability distribution over pages



#### $r = M \cdot r$ is Stationary Distribution

* Place for Surfer at $t+1$

  * Follow a link uniformly at random

    $p(t+1) = M \cdot p(t)$

  * Suppose the random walk reaches a steady state

    $p(t+1) = M \cdot p(t) = p(t)$

  * Then, $p(t)$ is **Stationary distribution** of random walk

* **Original Vector $r$** satisfies $r = M \cdot r$

  * So $r$ is a stationary distribution for random walk.



### Eigenvector Formulation

Lets look at flow equation
$$
r = M \cdot r
$$


when we restate $r$ as $1 \cdot r$,
$$
1 \cdot r = M \cdot r
$$
we can view 1 as eigenvalue and $r$ as eigenvector, with stochastic adjacency matrix $M$.

* Starting from any vector $u$, the limit $M(M(M...M(u)))$ is the **long-term distribution** of the surfer.
  * PageRank : Limiting distribution = Principal Eigenvector of $M$.
  * Note : if $r$ is the limit of product $M M M u$, then $r$ satisfies the flow equation $1 \cdot r = M \cdot r$.
  * So $r$ is the principal eigenvector of $M$ with eigenvaleu 1.
* This way we can solve for $r$ efficiently (better than gaussian elimination)
  * It is call **Power iteration**



-----



### Now, Solve PageRank with Power Iteration

* Given a graph(web) with $N$ node, we can use iterative procedure, power iteration.

  Nodes are pages and edges are hyperlink

  1. Assign each node an initial page rank(importance)
     * $r^0 = [1/N, 1/N\ ...\ , 1/N]^t$

  2. Repeat(iterate)
     * $r^{t+1} = M \cdot r^t$
     * About 30 iterations are sufficient

  3. Calculate the page rank of each node when it converges

     * $\sum_i\vert r_i^{t+1}-r_i^t\vert < \epsilon$

     > $\vert x \vert_1 = \sum^N_1 \vert x_i \vert$  is L1 norm
     >
     > Can use any other vector norm e.g. Euclidean, L2 norm...



#### Toy example

<img width="462" alt="image" src="https://user-images.githubusercontent.com/84625523/218247657-2a531bcc-b280-4e8d-9fe1-dea53c14e251.png">



1. Set $r_j \leftarrow 1/N$

2. $r'_j \leftarrow \sum_{i\rightarrow j} \frac{r_i}{d_i}$

3. If $\vert r - r' \vert < \epsilon$ : 

   â€‹	$r \leftarrow r'$

4. repeat 2 and 3

* Calculation of $[r_y\ r_a\ r_m]^T$:
  * Initial vector : $[\frac{1}{3}\ \frac{1}{3}\ \frac{1}{3}]^T$
  * 1st iteration :  $[\frac{1}{3}\ \frac{3}{6}\ \frac{1}{6}]^T$
  * 2nd iteration :  $[\frac{5}{12}\ \frac{1}{3}\ \frac{3}{12}]^T$
  * $n^{th}$ iteration :  $[\frac{6}{15}\ \frac{6}{15}\ \frac{3}{15}]^T$
    * Converges!



---



### PageRank's Problem and Solution



#### 1. Dead end

<img src="/Users/jojuhyeon/Library/Application Support/typora-user-images/image-20230211171602758.png" alt="image-20230211171602758" style="zoom:50%;" />

* No out-links

  * It cause importance to "leak out"

  * $[r_a\ r_b] \rightarrow [1\ 0] \rightarrow [0\ 1] \rightarrow [0\ 0]\ \rightarrow [0\ 0]\ ...$



#### Dead end's Solution : Teleport

* Dead end fallows random teleport links with total probability 1.0
  * New path for dead end node

<img src="/Users/jojuhyeon/Library/Application Support/typora-user-images/image-20230211171948938.png" alt="image-20230211171948938" style="zoom:40%;" />





#### 2. Spider traps

<img src="/Users/jojuhyeon/Library/Application Support/typora-user-images/image-20230211171552137.png" alt="image-20230211171552137" style="zoom:50%;" />

* All out-links are within the group
  * Eventually spider traps absorb all importance
  * $[r_a\ r_b] \rightarrow [1\ 0] \rightarrow [0\ 1] \rightarrow [0\ 1]\ \rightarrow [0\ 1]\ ...$



#### Spider trap's solution : Again, teleport

* At each time step, random surfer has two option

  1. With prob. $\beta$, follow a link at random (no adjustment)
  2. With prob. $1- \beta$, teleport to random page.

  * Common $\beta$ are in range of 0.8 to 0.9

* By this trick, surfer will teleport out of spider trap within few more time steps



### Why teleport?

* First of all, spider trap **is not a problem, but score with trap is not what we want**
  * Sol : Don't get stuck by spider trap by teleporting within finite attempts.
* Dead-end **Is a problem**
  * With dead end, matrix **is not column stochastic**, so our initial assumptions are not match, i.e. Does not converge.
  * Sol : Make matrix column stochastic by **always teleporting to other node randomly**



----



### Google's solution for Random Teleport

* At each(and every) step, random surfer has two option

  1. With prob. $\beta$, follow a link at random (no adjustment)
  2. With prob. $1- \beta$, teleport to random page.

* New equation follows,
  $$
  r_j = \sum_{i\rightarrow j} \beta \frac{r_i}{d_i} + (1- \beta)\frac{1}{N}
  $$

  > Brin-Page, '98

  * This formulation assumes $M$ has no dead-ends.
    * Preprocess $M$ to remove all dead ends or
    * explicitly follow random teleport links with prob. 1.0 from dead end



#### Google Matrix

$$
G = \beta M+ (1-\beta)[\frac1{N}]_{N\times N}
$$

* Recursive problem : $r = G \cdot r$



#### Example

<img width="923" alt="image" src="https://user-images.githubusercontent.com/84625523/218248841-5fa16e7d-6ca6-4d84-92bd-96ff1632961b.png" style="zoom:50%;" >





<img src="/Users/jojuhyeon/Library/Application Support/typora-user-images/image-20230211173859682.png" alt="image-20230211173859682" style="zoom: 33%;" />

* We can see there are non-zero importance in any node
* B has many in-links $\rightarrow$ it has higher importance(rank)
  * Also most of in-links of B has high importance
* E has in-link that has low importance $\rightarrow$ lower importance
* C has one in-links from B, which has high importance $\rightarrow$ high importance

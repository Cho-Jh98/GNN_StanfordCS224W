# 3. Node embedding



* Goal : Efficient task-independent feature learning for machine learning with graph.
  * Function $f$ that makes feature vector out of graph : $f : u \rightarrow \mathbb{R}^d$



* Task : Map nodes into an embedding space

  * Similarity of embeddings between nodes indicates their similarity in the network.
    * EX) Both nodes are close to each other (cosine similarity)
  * Encode network(graph) information
  * Potentially used for many downstream prediction
    * Node classification
    * Link prediction
    * Graph classification
    * Anomalous node detection
    * Clustering

* Example of node embedding with Zachary's Karata Club network

  <img width="1514" alt="image" src="https://user-images.githubusercontent.com/84625523/211789690-cadf9a68-f5df-405b-974e-88f212f851d4.png" style="zoom:50%;" >



## Embedding nodes

### Setup

* $V$ is vertex set, $A$ is adjacency matrix, assume binary.
  * For simplicity, no node features or extra information is used, and undirected.



### Now, embedding

* Goal is to encode node, so that similarity in the embedding space that approximates similarity in the graph.
  * EX) dot product or distance
* Goal is to define function for ecoding from graph to embedding space that fallows..
  * $\text{similarity}(u, v)\approx z_v^Tz_u$
  * similiarity in the original network approximates similarity of the embedding space.



### Steps

1. Encoder maps from nodes to embeddings
2. Define a node similarity function
   * EX) measure of similarity in the original network
3. Decoder DEC maps from embeddings to the similarity score
4. Optimize the parameter of the encoder so that :  $\text{similarity}(u, v)\approx z_v^Tz_u$



-------



### Key components : Encoder and Similarity Function

#### Encoder

* Maps each node to a low-dimensional vector
* $ENC(v) = z_v$
  * $v$ is node in input graph
  * $z_v$ is $d$-dimensional embedding

#### Similarity function

* Specifies how the relationship in vector space map to the relationships in the original network.





### Shallow encoding 

* Simplest encoding approach : **Encoder is just an embedding-lookup**
* $ENC(v) = z_v = Z\cdot v$
  * $Z \in \mathbb{R}^{d \times \vert V\vert}$ 
    * matrix, each column is node embedding
    * These are what we learn / optimize
  * $v \in \mathbb{I}^{\vert V \vert}$ 
    * indicator vector, all zeroes except a one in column indicating node $v$

<img width="1623" alt="image" src="https://user-images.githubusercontent.com/84625523/211792603-a0c01e64-b9f4-46ca-a101-efb8c09e5e34.png" style="zoom:40%;" >

* Each node is assigned a unique embedding vector
  * i.e. we directly optimize the embedding of each node
* Deepwalk, node2vec and others..





### Framework : Encoder + Decoder

* Shallow encoder : embedding lookup
* Parameter to optimize: $Z$
  * $Z$ contains node embedding $z_u$ for all nodes $u \in V$
* Decoder : based on node similiarity
* Objective : Maximize $z_v^Tz_u$ for node pairs $(u, v)$ that are **similar**
  * But how to define node similarity?
  * Answer is **Random Walks**





### Simple note on node embedding

* This is **unsupervise / self-supervised** way
  * Not utilizing node label
  * Not utilizing node feature
  * Goal is to directly estimate a set of coordinates of a node, So that some aspect of the network structure is preserved
    * the aspect of network will be captured by DEC
* So these embeddings are **task independent**
  * Not trained for a specific tast but can be used for any task.



----



## Random walk

<img width="1515" alt="image" src="https://user-images.githubusercontent.com/84625523/211795307-06e3a1be-49d5-4a9f-993f-ddce244bc2fa.png" style="zoom:40%;" >



### Notation

* Vector $z_u$ 
  *  embedding of node $u$ (what we aim to find)
* Probabilty $P(v \vert z_u)$
  * The (predicted) probability of visiting node $v$ on random walks starting from $u$.



* **Softmax** Function
  * Turns vector of $K$ real values (model prediction) into $K$ probabilities that sum to 1
  * $\sigma(z)[i] = \frac{e^{z[i]}}{\sum_{j=1}^K e^{z[j]}}$
* **Sigmoid** Function
  * S-shaped function that turns real value into the range of (0, 1)
  * $S(x) = \frac{1}{1 + e^{-x}}$



### Random walk embeddings



* $z_u^Tz_v \approx$  Probability that $u$ and $v$ co-occur on a random walk over the graph



<img width="448" alt="image" src="https://user-images.githubusercontent.com/84625523/211796276-fd505747-e932-4ec0-b3f5-6286979bb5a7.png" style="zoom: 80%;" > 

1. Estimate probability of visiting node $v$ on a random walk starting from node $u$ using some random walk strategy $R$



<img width="412" alt="image" src="https://user-images.githubusercontent.com/84625523/211796431-2cfc5838-6e08-4452-9ad6-c62c831ebb1e.png" style="zoom:80%;" > 

2. Optimize embedding to encode these random walk statistics
   * Similarity in embedding space encodes random walk "similarity."



### Why random walks?

* Expressivity
  * Flexible stochastic definition of node similarity that **incorporates both local and higher-order neighborhood information**
  * Idea : if random walk starting from node $u$ visits $v$ with high probability, $u$ and $v$ are similar
    * High-order multi-hop information
* Efficiency
  * Do not need to consider all node pairs when training
  * **Only need to consider pairs that co-occur on random walks**



---------



## Feature learning

* Intuition : Find embedding of nodes in $d$-dimensional space that preserve similarity
* Idea : Learn node embedding such that **nearby** nodes are close together in the network



* $N_R(u)$ : Neighborhood of $u$ obtained by some **random walk strategy R**
  * Define nearby nodes of $u$



### Random walk optimization

* Given $G = (V, E)$
* Goal : Find function $f : u \rightarrow \mathbb{R}^d$ : $f(u) = z_u$
* Log-likelihood objective:


$$
\max_f \sum_{u\in V} \log(N_R(u)\vert z_u)
\\
\\
\text{$N_R(u)$ is the neighborhood of node $u$ by strategy $R$}
$$


* **Given node $u$, we want to find feature representation that are predictive of the nodes in its random walk neighborhood $N_R(u)$.**



### How?

1. Run **short, fixed-length random walks** starting from each node $u$ in the graph, using some random walk strategy $R$
2. For each node $u$, collect $N_R(u)$, the multiset of node visited on random walks starting from $u$.
3. Optimize embeddings according to **Given node $u$, predict its neighbors $N_R(u)$**


$$
\max_f \sum_{u\in V} \log(N_R(u)\vert z_u) \rightarrow \text{maximum likelihood objective}
$$



### Also, 


$$
\mathcal{L} = \sum_{u\in V}\sum_{v \in N_R(u)} -\log(P(v \vert z_u))
$$



* Intuition : Optimize embeddings $z_u$ to maximize the likelyhood of random walk co-occurrences.
* Parameterize $P(v\vert z_u)$ using softmax: 


$$
P(v \vert z_u) = \frac{\exp(z_u^Tz_v)}{\sum_{n \in V}\exp(z_u^Tz_n)}
$$



### So, put it all together


$$
\mathcal{L} = \sum_{u\in V}\sum_{v \in N_R(u)} -\log(\frac{\exp(z_u^Tz_v)}{\sum_{n \in V}\exp(z_u^Tz_n)})
$$





* $\sum_{u\in V}$ : sum over all nodes $u$
* $\sum_{v \in N_R(u)}$ : sum over node $v$ seen on random walks starting from $u$
* $\frac{\exp(z_u^Tz_v)}{\sum_{n \in V}\exp(z_u^Tz_n)}$ : predicted probability of $u$ and $v$ co-occuring on random walk.



#### Optimizing random walk embeddings = Finding embedding $z_u$ that Minimize $\mathcal{L}$



------



## Poblem, Too Expensive!

$\sum_{u\in V}$  and also  $\sum_{n \in V}\exp(z_u^Tz_n)$  leads to $O(\vert V\vert^2)$ complexity!

* Normalization term from softmax is the culprit.



### Answer, Negative Sampling


$$
\log(\frac{\exp(z_u^Tz_v)}{\sum_{n \in V}\exp(z_u^Tz_n)})
\\
\\
\approx \log(\sigma(z_u^Tz_v)) - \sum_{i=1}^k \log(\sigma(z_u^Tz_i)), \ n_i \sim P_V
$$



* $\sigma()$ : sigmoid function
* $n_i \sim P_v$ : random distribution over nodes
* Instead of normalizing w.r.t. all nodes, just normalize against $k$ random "**Negative Sample**", $n_i$ 
  * Negative sampling allows for quick likelihood calculation.
  * Sample $k$ negative nodes each with probability, proportional to its degree
    * the higher degree the node has, the more chance to be chosen
    * Two consideration to choose $k$
      1. Higher $k$ gives more robust estimates
      2. Higher $k$ corresponds to higher bais on negative events.
    * In practice, $k$ is 5 ~ 20.



### Last but not least, SGD(Stochastic Gradient Descent)

* Instead of evaluating gradients over all examples, evaluate it for each individual training example.
  1. Initialize $z_u$ at some randomized value for all $u$
  2. Iterate untile convergence: $\mathcal{L} = \sum_{v \in N_R(u)}\log(P(V\vert z_u))$.
     * Sample a node $u$, for all $v$ calculate the derivative $\frac{\partial\mathcal{L}^{(u)}}{\partial z_v}$.
     * for all $v$, update: $z_v \leftarrow \eta\frac{\partial\mathcal{L}^{(u)}}{\partial z_v}$.



* Now remaining question is strategy for random walk.

* Simple idea would be **Just run fixed-length, unbiased random walk starting at each node**
  * This is proposed by DeepWalk from Perozzi et al.
  * The issue was that such notion of similarity is too constrained
    * i.e. we need to generalize this. Make it more expressive.



-----



## Random walk strategy

* More generalized, more expressive strategy of random walk is node2vec



### Node2vec

* Goal : Embed nodes with similar network neighborhood close in the feature space.
  * Frame this goal as maximum likelyhood optimization problem. Independent to the downstream prediction task.
* Observation : Flexible notion of network neighborhood $N_R(u)$ of node $u$ can leads to rich node embeddings.
* Develop biased $2^{\text{nd}}$ order random walk to $R$, to generate network neighborhood $N_R(u)$ of $u$.
  * What is **Biased**, **$2^{\text{nd}}$ order random walk**?





### Biased Walks (BFS and DFS)

* Idea is that using flexible and biased random walks that can trade of between **Local** and **Global** views of network.

<img width="1172" alt="image" src="https://user-images.githubusercontent.com/84625523/212352215-7d554a9c-de5f-4469-b0d0-e79e2e43d642.png" style="zoom:50%;" >

* Here, two classic strategies to define a neighborhood $N_R(u)$ of given node $u$:

  * $N_{BFS}(u) = \{s_1, s_2, s_3\}$  :  **Local** microscopic view
  * $N_{DFS}(u) = \{s_4, s_5, s_6\}$  :  **Global** macroscopic view

* We are going to interpolate between BFS and DFS

* Baised fixed-length random walk $R$ that given a node $u$ generate neighborhood $N_R(u)$.

  * There is two parameter for 'Biased' walk

    * Return parameter $p$ :

      Return back to the previous node

    * In-out parameter $q$  :

      Moving outwards (DFS)  vs.  inward (BFS)

      $q$ is the 'Ratio' of BFS  vs.  DFS





### Now $2^{\text{nd}}$-order random walk kicks in

* $2^{\text{nd}}$-order random walks explore network neighborhood.



Below is example for random walk, starting from $u$ and now currently at $s_1$.

<img width="1097" alt="image" src="https://user-images.githubusercontent.com/84625523/212354196-adf5a4d7-76c4-4130-b55e-4213ef882d4a.png" style="zoom:50%;" >

* Imagine random walk just traversed edge $(s_1, w)$ and is now at $w$

  * Insight : **Neighbors of $w$ can only be**
    1. Back to $s_1$
    2. Same distance to $s_1$
    3. Farther from $s_1$
  * When we take a next step for random walk, we are going to assign transition probabilities

  <img width="906" alt="image" src="https://user-images.githubusercontent.com/84625523/212355058-833ee180-8caa-49c8-a8e7-90675e68f54d.png" style="zoom:50%;" >

  * $p$, $q$ are transition probabilities(Unnormalized. Need to normalize later)
    * $p$ : return parameter
    * $q$ : "walk away" parameter
  * If we want **BFS-like** walk, assign low value of $p$, otherwise for **DFS-like** walk put low value on $q$.





### Complete node2vec algorithm

1) Compute random walk probabilities $p \text{ and } q$.
2) Simulate $r$ random walks of length $l$ starting from each node $u$.
3) Optimize the node2vec objective using SGD



#### Pros

* **Linear-time complexity**
* **Individually parallelizable**



#### Cons

* Learn separate embedding for every node
  * Bigger graph needs learning bigger embedding.



-----



## Embedding of (sub)Graph

* Now we want to embed a whole (sub)graph $G$.
  * Graph embedding : $z_G$
  * Tasks : 
    * Classifying toxic versus non-toxic molecules
    * Identifying anomalous graphs.



### Approach 1

* Simple, but effective approach
  1. Run a standard graph embedding technique on the (sub)graph $G$.
  2. Sum (or average) the node embeddings in the (sub)graph $G$.


$$
z_G = \sum_{v \in G} z_v
$$


* This approach is used to classify molecules based on their graph structure





### Approach 2

* Introduce a **virtual node** to represent the (sub)graph and run a standard graph embedding technique.

<img width="1252" alt="image" src="https://user-images.githubusercontent.com/84625523/212359791-5d70df41-35a7-4de5-826f-d82c81fededb.png" style="zoom:50%;" >





### Approach 3

States in **anonymous walks** correspond to the index of the **first time** we visited the node in random walk.

<img width="741" alt="image" src="https://user-images.githubusercontent.com/84625523/212360050-d3d2a040-4592-4eb3-9724-49d6cc521aff.png" style="zoom:67%;" >

* In Random Walk 1
  * Node A, B, C has index of A - 1, B - 2, C - 3, hence anonymous walk would be 1 2 3 2 3
* In Random Walk 2
  * Node B, C, D has index of C - 1, D - 2, B - 3, hence anonymous walk would be 1 2 3 2 3
* In Random Walk 3
  * Node A, B, D has index of A - 1, B - 2, D - 3, hence anonymous walk would be 1 2 1 2 3



<img width="1023" alt="image" src="https://user-images.githubusercontent.com/84625523/212361747-d0ee293e-1591-4aa7-b6bc-695f28b736e7.png" style="zoom:50%;" >

* As we can see at the graph, number of anonymous walks grows exponentially as length gets bigger.
  * EX) With walks of $w_i$ of length 3, there is 5 anonymous walks
    * $w_1$ = 111, $w_2$ = 112, $w_3$ = 121, $w_4$ = 122, $w_5$ = 123





#### How to use?

* Simulate anonymous walks $w_i$ of $l$ steps and record their counts.
* **Represent the graph as a probability distribution over these walks**



##### Example

1. Set $l =3$.
2. Then, we can represent the graph as 5-dim vector.
   * Since there is 5 anonymous walks of $w_i$ of length 3 : 111, 112, 121, 122, 123
3. $z_G[i]$ = probability of anonymous walk $w_i$ in graph $G$.



* **Sampling anonymous walks**
  * Generate independently a set of $m$ random walks.
* **Represent the graph as a probabilty distribution over these walks.**



##### How many random walks $m$ do we need?

* Distribution to have error more than $\varepsilon$ with probabilty less than $\delta$ :


$$
m = \lceil \frac{2}{\varepsilon^2}(\log(2^\eta -2)-\log(\delta))\rceil
\\
\\
\text{$\eta$ : total number of anonymous walks of length $l$.}
$$


---



## Learning the Walk Embedding

* Rather than simply representing each walk by the fraction of times it occurs, we **Learn embedding $z_i$ of anonymous walk $w_i$**
  * Learn a graph embedding $z_G$ together with all the anonymous walk embedding $z_i$.
  * $Z = \{z_i : i = 1, 2, ... \eta\}$, where $\eta$ is the number of sampled anom. walks.
* How? Embed walks such that the next walk can be predicted.
  * Consider past of random walks.



* Output is a vector $z_G$ for input graph of $G$. 
  * The embedding of entire graph to be learned.



#### Procedure

1. Starting from node 1, sample anonymous random walk.
2. Learn to predict walks that co-occur in $\Delta$-size window.
   * EX) predict $w_3$ given $w_1$, $w_2$ if $\Delta$ = 2.
3. Objective : 


$$
\max_{z_G} \sum^T_{t=\Delta+1}\log P(w_t \vert w_{t-\Delta}, w_{t-\Delta-1},\ ...\ w_{t-1},\  z_G)
$$


* Where $T$ is the total number of walks





* Run $T$ different random walks from $u$, each of length $l$ : $N_R(u) = \{ w_1^u, w_2^u, ... , w_T^u\}$
* And learn to predict walks that co-occur in $\Delta$-size window
  * Estimate embeddding $z_i$ of anonymous walk $w_i$.
  * Let $\eta$ be the number of all possible walk embeddings.


$$
\text{Objective : }\max_{z_i, z_G}\frac{1}{T}\sum^T_{t = \Delta}\log P(w_t \vert w_{t-\Delta}, w_{t-\Delta-1},\ ...\ w_{t-1},\  z_G)
$$


$$
P(w_t \vert w_{t-\Delta}, w_{t-\Delta-1},\ ...\ w_{t-1},\  z_G) = \frac{\exp(y(w_t))}{\sum_{i=1}^{\eta} \exp(y(w_i))}
$$


$$
y(w_t) = b + U \cdot(cat(\frac1\Delta \sum^{\Delta}_{i=1} z_i, z_G))
$$


* $cat(\frac{1}{\Delta} \sum^{\Delta}_{i=1} z_i, z_G))$ means an average of anonymous walk embeddings in $z_i$ in the window, concatenated with the graph embedding $z_G$.
* $b \in \mathbb{U}$, $U \in \mathbb{R}^D$ are learnable parameters.
  * This represent a linear layer.



* We obtain the graph embedding $z_G$ (learnable parameter) after the optimization.
  * According to paper, $z_G$ is a separately optimized vector parameter, just like other $z_i$'s.

* Use $z_G$ to make predictions
  * Option 1 : Inner product Kernel $z_{G_1}^{T}z_{G_2}$
  * Option 2 : Use neural network that take $z_G$ as input to classification.





















